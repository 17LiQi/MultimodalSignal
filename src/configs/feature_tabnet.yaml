# configs/feature_tabnet.yaml

defaults:
  - base_config

dataset:
  name: "wesad_feature_fusion"
  channels_to_use: null

model:
  name: "tabnet"
  params:
    in_features: 24 # 确认这个值与 preprocess_check.py 的输出一致
    num_classes: 3

    # --- TabNet 自身的模型结构超参数 ---
    # 这些参数直接传递给 TabNetClassifier 的构造函数
    tabnet_params:
      # n_d 和 n_a 定义了模型的核心维度。较大的值意味着更强的表达能力，但也更容易过拟合。
      # 对于特征数较少 (24) 的情况，8 或 16 是一个很好的起点。
      n_d: 8          # 决策步输出的特征维度 (Decision step output dimension)
      n_a: 8          # 注意力步输出的特征维度 (Attention step output dimension)

      # n_steps 控制模型的“深度”，即有多少个连续的决策/注意力步骤。
      # 3-5 步是常见选择。步数越多，模型越复杂。
      n_steps: 4       # 决策步的数量

      # gamma 是一个正则化参数，鼓励注意力掩码的稀疏性。
      # 值大于 1.0 会使得模型在每个决策步倾向于选择更少的特征。1.3 - 1.5 是常用值。
      gamma: 1.5

      # n_independent 和 n_shared 控制每个决策步中 Gated Linear Unit (GLU) 层的数量。
      # 它们共同决定了模型的宽度。
      n_independent: 2 # 独立 GLU 层的数量
      n_shared: 2      # 共享 GLU 层的数量

      # mask_type 决定了注意力机制的类型。
      # 'sparsemax' (推荐) 会产生真正的稀疏掩码 (很多权重为0)，使得特征选择更具可解释性。
      # 'entmax' 是一个介于 sparsemax 和 softmax 之间的选择。
      mask_type: 'sparsemax'

# --- 训练器配置 (覆盖 base_config) ---
trainer:
  epochs: 100 # TabNet 自带早停
  batch_size: 512 # TabNet 对大 batch size 友好，可以加速训练并起到正则化作用

  # 早停参数将传递给 TabNet 模型的 .fit() 方法
  early_stopping:
    enabled: true # 确保启用
    patience: 20  # 如果验证集性能20轮不提升则停止
    monitor: 'val_loss'
    mode: 'min'

  # 是否使用类别加权 (对于 TabNet，可以先设为 false，看其自身性能如何)
  use_class_weights: false

  # --- 优化器和学习率调度器的参数 ---
  # 这些参数将传递给 TabNetClassifier 的构造函数
  optimizer_params:
    # AdamW 是一种带有权重衰减的 Adam 优化器，通常比标准 Adam 表现更好
    # optimizer_fn: torch.optim.AdamW
    lr: 0.005 # TabNet 论文推荐的学习率通常比传统CNN高，但在小数据集上表现可能不太好

  scheduler_params:
    # ReduceLROnPlateau 的参数
    mode: 'min'
    patience: 10   # 如果验证损失5轮不下降，则降低学习率
    factor: 0.2   # 学习率乘以 0.2

# --- 融合方式配置 ---
fusion:
  type: "feature"